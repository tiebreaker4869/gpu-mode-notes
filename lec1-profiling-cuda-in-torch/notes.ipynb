{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3193bfe3",
   "metadata": {},
   "source": [
    "# Profiling CUDA in Torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976131e9",
   "metadata": {},
   "source": [
    "## TL; DR\n",
    "\n",
    "当需要定制化 cuda kernel 并且整合到 torch 代码中时，我们主要关心两个问题:\n",
    "1. 如何对 cuda kernel 做 profiling (这也有助于我们决定是否需要自己实现 cuda kernel, 我们的目标是在真正需要的地方自己实现定制化的 kernel, 并且效率高于编译技术生成的实现)\n",
    "2. 如何在 torch 代码中使用定制化的 cuda kernel.\n",
    "\n",
    "对于 profiling，主要的工具有：\n",
    "- cuda event\n",
    "- torch.autograd.profiler\n",
    "- torch.profiler\n",
    "- ncu\n",
    "\n",
    "对于如何在 torch 中使用定制化的 cuda kernel，主要有两种比较简单的方式：\n",
    "- 使用 triton 实现 kernel (然后可以直接使用，triton kernel 就是一个 python 函数加了 @triton.jit);\n",
    "- 使用 torch.utils.cpp_extension 的 load_inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5058a92c",
   "metadata": {},
   "source": [
    "## Profiling CUDA Kernel in torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed95b191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ce292a",
   "metadata": {},
   "source": [
    "### CUDA Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5032c39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_pytorch_function(f, input):\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    for _ in range(10):\n",
    "        # warmup\n",
    "        f(input)\n",
    "    \n",
    "    start.record()\n",
    "    f(input)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    return start.elapsed_time(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c97fcf23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1735039949417114"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randn((10000, 10000)).cuda()\n",
    "time_pytorch_function(torch.square, t) # in milliseconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb597b39",
   "metadata": {},
   "source": [
    "### torch.autograd.profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0278ad43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "             aten::square         1.29%      15.811us         7.39%      90.492us      90.492us      14.000us         1.12%       1.253ms       1.253ms             1  \n",
      "                aten::pow         4.33%      52.971us         5.71%      69.973us      69.973us       1.233ms        98.40%       1.239ms       1.239ms             1  \n",
      "        aten::result_type         0.12%       1.422us         0.12%       1.422us       1.422us       4.000us         0.32%       4.000us       4.000us             1  \n",
      "                 aten::to         0.03%       0.411us         0.03%       0.411us       0.411us       2.000us         0.16%       2.000us       2.000us             1  \n",
      "          cudaEventRecord         1.61%      19.717us         1.61%      19.717us       2.465us       0.000us         0.00%       0.000us       0.000us             8  \n",
      "         cudaLaunchKernel         0.70%       8.597us         0.70%       8.597us       8.597us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "    cudaDeviceSynchronize        91.92%       1.126ms        91.92%       1.126ms       1.126ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.225ms\n",
      "Self CUDA time total: 1.253ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.autograd.profiler.profile(use_device='cuda') as prof:\n",
    "    torch.square(t)\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "# the result denotes most task is done in aten::pow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948644ce",
   "metadata": {},
   "source": [
    "### torch.profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a4adbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                          ProfilerStep*         0.00%       0.000us         0.00%       0.000us       0.000us     103.404ms       234.41%     103.404ms      51.702ms             2  \n",
      "                                            aten::copy_         0.01%      60.255us         3.66%      42.133ms      21.067ms      41.776ms        94.70%      41.776ms      20.888ms             2  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      41.776ms        94.70%      41.776ms      20.888ms             2  \n",
      "                                              aten::pow         0.01%      89.419us         0.01%     151.878us      75.939us       2.338ms         5.30%       2.338ms       1.169ms             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.338ms         5.30%       2.338ms       1.169ms             2  \n",
      "                                          ProfilerStep*         5.16%      59.420ms        99.91%        1.150s     574.768ms       0.000us         0.00%      44.113ms      22.057ms             2  \n",
      "                                            aten::randn         0.00%      48.751us        91.06%        1.048s     523.836ms       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                            aten::empty         0.00%      44.515us         0.00%      44.515us      22.257us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                          aten::normal_        91.05%        1.048s        91.05%        1.048s     523.790ms       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                               aten::to         0.00%      57.218us         3.68%      42.285ms      10.571ms       0.000us         0.00%      41.776ms      10.444ms             4  \n",
      "                                         aten::_to_copy         0.00%      40.888us         3.67%      42.228ms      21.114ms       0.000us         0.00%      41.776ms      20.888ms             2  \n",
      "                                    aten::empty_strided         0.00%      53.862us         0.00%      53.862us      26.931us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                        cudaMemcpyAsync         3.65%      42.004ms         3.65%      42.004ms      21.002ms       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                  cudaStreamSynchronize         0.01%      68.610us         0.01%      68.610us      34.305us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                           aten::square         0.00%       7.435us         0.01%     159.313us      79.657us       0.000us         0.00%       2.338ms       1.169ms             2  \n",
      "                                      aten::result_type         0.00%       2.835us         0.00%       2.835us       1.418us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                       cudaLaunchKernel         0.01%      58.442us         0.01%      58.442us      29.221us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                  cudaDeviceSynchronize         0.09%       1.021ms         0.09%       1.021ms       1.021ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.151s\n",
      "Self CUDA time total: 44.113ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ## Default way to use profiler\n",
    "# with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "#     for _ in range(10):\n",
    "#         a = torch.square(torch.randn(10000, 10000).cuda())\n",
    "\n",
    "# prof.export_chrome_trace(\"trace.json\")\n",
    "\n",
    "## With warmup and skip\n",
    "# https://pytorch.org/docs/stable/profiler.html\n",
    "\n",
    "# Non-default profiler schedule allows user to turn profiler on and off\n",
    "# on different iterations of the training loop;\n",
    "# trace_handler is called every time a new trace becomes available\n",
    "def trace_handler(prof):\n",
    "    print(prof.key_averages().table(\n",
    "        sort_by=\"self_cuda_time_total\", row_limit=-1))\n",
    "    prof.export_chrome_trace(\"/tmp/test_trace_\" + str(prof.step_num) + \".json\")\n",
    "\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ],\n",
    "\n",
    "    # In this example with wait=1, warmup=1, active=2, repeat=1,\n",
    "    # profiler will skip the first step/iteration,\n",
    "    # start warming up on the second, record\n",
    "    # the third and the forth iterations,\n",
    "    # after which the trace will become available\n",
    "    # and on_trace_ready (when set) is called;\n",
    "    # the cycle repeats starting with the next step\n",
    "\n",
    "    schedule=torch.profiler.schedule(\n",
    "        wait=1,\n",
    "        warmup=1,\n",
    "        active=2,\n",
    "        repeat=1),\n",
    "    on_trace_ready=trace_handler\n",
    "    # on_trace_ready=torch.profiler.tensorboard_trace_handler('./log')\n",
    "    # used when outputting for tensorboard\n",
    "    ) as p:\n",
    "        for iter in range(10):\n",
    "            torch.square(torch.randn(10000, 10000).cuda())\n",
    "            # send a signal to the profiler that the next iteration has started\n",
    "            p.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad267657",
   "metadata": {},
   "source": [
    "### ncu: NVIDIA Nsight Compute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe60ff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ncu --set full -o output $(which python) test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc93d61",
   "metadata": {},
   "source": [
    "## Integrating Custom kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bfa817",
   "metadata": {},
   "source": [
    "### Triton Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d67cb09",
   "metadata": {},
   "source": [
    "### Load inline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
