{"cells":[{"cell_type":"markdown","id":"3193bfe3","metadata":{"id":"3193bfe3"},"source":["# Profiling CUDA in Torch"]},{"cell_type":"code","source":["!pip install ninja"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dpu1ZiVR42m9","executionInfo":{"status":"ok","timestamp":1749358696054,"user_tz":420,"elapsed":4339,"user":{"displayName":"Lifan Sun (Steven)","userId":"05795298000904000024"}},"outputId":"e0d0e410-dae0-4cde-e10c-ebe37a9bb836"},"id":"Dpu1ZiVR42m9","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (1.11.1.4)\n"]}]},{"cell_type":"code","source":["!sudo apt update\n","!sudo apt install build-essential ninja-build"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iG7tWcmk5nlq","executionInfo":{"status":"ok","timestamp":1749358706462,"user_tz":420,"elapsed":10396,"user":{"displayName":"Lifan Sun (Steven)","userId":"05795298000904000024"}},"outputId":"13d2d5cc-cfbf-46c8-8698-db34b70849ee"},"id":"iG7tWcmk5nlq","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n","Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n","Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n","Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n","Hit:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n","Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Hit:9 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n","Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","\n","Building dependency tree... Done\n","Reading state information... Done\n","36 packages can be upgraded. Run 'apt list --upgradable' to see them.\n","\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","build-essential is already the newest version (12.9ubuntu3).\n","ninja-build is already the newest version (1.10.1-1).\n","0 upgraded, 0 newly installed, 0 to remove and 36 not upgraded.\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vj4OhNadeC6k","executionInfo":{"status":"ok","timestamp":1749358707185,"user_tz":420,"elapsed":721,"user":{"displayName":"Lifan Sun (Steven)","userId":"05795298000904000024"}},"outputId":"b503116a-e68f-48a9-9ad8-307b0851875b"},"id":"vj4OhNadeC6k","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/ongoing/gpu-mode/gpu-mode-notes/lec1-profiling-cuda-in-torch/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-2kRfxY6yyNV","executionInfo":{"status":"ok","timestamp":1749358707203,"user_tz":420,"elapsed":15,"user":{"displayName":"Lifan Sun (Steven)","userId":"05795298000904000024"}},"outputId":"330dfea4-3b84-45cc-cb2f-77f87cfaad6d"},"id":"-2kRfxY6yyNV","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/ongoing/gpu-mode/gpu-mode-notes/lec1-profiling-cuda-in-torch\n"]}]},{"cell_type":"markdown","id":"976131e9","metadata":{"id":"976131e9"},"source":["## TL; DR\n","\n","当需要定制化 cuda kernel 并且整合到 torch 代码中时，我们主要关心两个问题:\n","1. 如何对 cuda kernel 做 profiling (这也有助于我们决定是否需要自己实现 cuda kernel, 我们的目标是在真正需要的地方自己实现定制化的 kernel, 并且效率高于编译技术生成的实现)\n","2. 如何在 torch 代码中使用定制化的 cuda kernel.\n","\n","对于 profiling，主要的工具有：\n","- cuda event\n","- torch.autograd.profiler\n","- torch.profiler\n","- ncu\n","\n","对于如何在 torch 中使用定制化的 cuda kernel，主要有两种比较简单的方式：\n","- 使用 triton 实现 kernel (然后可以直接使用，triton kernel 就是一个 python 函数加了 @triton.jit);\n","- 使用 torch.utils.cpp_extension 的 load_inline"]},{"cell_type":"markdown","id":"5058a92c","metadata":{"id":"5058a92c"},"source":["## Profiling CUDA Kernel in torch"]},{"cell_type":"code","execution_count":5,"id":"ed95b191","metadata":{"id":"ed95b191","executionInfo":{"status":"ok","timestamp":1749358709586,"user_tz":420,"elapsed":2382,"user":{"displayName":"Lifan Sun (Steven)","userId":"05795298000904000024"}}},"outputs":[],"source":["import torch"]},{"cell_type":"markdown","id":"44ce292a","metadata":{"id":"44ce292a"},"source":["### CUDA Event"]},{"cell_type":"code","execution_count":6,"id":"5032c39a","metadata":{"id":"5032c39a","executionInfo":{"status":"ok","timestamp":1749358709593,"user_tz":420,"elapsed":3,"user":{"displayName":"Lifan Sun (Steven)","userId":"05795298000904000024"}}},"outputs":[],"source":["def time_pytorch_function(f, input):\n","    start = torch.cuda.Event(enable_timing=True)\n","    end = torch.cuda.Event(enable_timing=True)\n","\n","    for _ in range(10):\n","        # warmup\n","        f(input)\n","\n","    start.record()\n","    f(input)\n","    end.record()\n","    torch.cuda.synchronize()\n","\n","    return start.elapsed_time(end)"]},{"cell_type":"code","execution_count":7,"id":"c97fcf23","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c97fcf23","executionInfo":{"status":"ok","timestamp":1749358711716,"user_tz":420,"elapsed":2118,"user":{"displayName":"Lifan Sun (Steven)","userId":"05795298000904000024"}},"outputId":"abfbadf7-ce18-41eb-87b9-d8cafa1ac34f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["3.227776050567627"]},"metadata":{},"execution_count":7}],"source":["t = torch.randn((10000, 10000)).cuda()\n","time_pytorch_function(torch.square, t) # in milliseconds"]},{"cell_type":"markdown","id":"fb597b39","metadata":{"id":"fb597b39"},"source":["### torch.autograd.profiler"]},{"cell_type":"code","execution_count":8,"id":"0278ad43","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0278ad43","executionInfo":{"status":"ok","timestamp":1749358711754,"user_tz":420,"elapsed":36,"user":{"displayName":"Lifan Sun (Steven)","userId":"05795298000904000024"}},"outputId":"d57ea7f7-08e3-416f-e9db-552fec7e04c0"},"outputs":[{"output_type":"stream","name":"stdout","text":["-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n","-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","             aten::square         0.36%      35.288us         1.74%     168.391us     168.391us      37.000us         1.10%       3.369ms       3.369ms             1  \n","                aten::pow         0.98%      94.657us         1.30%     125.441us     125.441us       3.320ms        98.55%       3.332ms       3.332ms             1  \n","        aten::result_type         0.03%       2.590us         0.03%       2.590us       2.590us       7.000us         0.21%       7.000us       7.000us             1  \n","                 aten::to         0.01%       0.808us         0.01%       0.808us       0.808us       5.000us         0.15%       5.000us       5.000us             1  \n","          cudaEventRecord         0.34%      32.553us         0.34%      32.553us       4.069us       0.000us         0.00%       0.000us       0.000us             8  \n","         cudaLaunchKernel         0.18%      17.452us         0.18%      17.452us      17.452us       0.000us         0.00%       0.000us       0.000us             1  \n","    cudaDeviceSynchronize        98.10%       9.491ms        98.10%       9.491ms       9.491ms       0.000us         0.00%       0.000us       0.000us             1  \n","-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","Self CPU time total: 9.674ms\n","Self CUDA time total: 3.369ms\n","\n"]}],"source":["with torch.autograd.profiler.profile(use_device='cuda') as prof:\n","    torch.square(t)\n","print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n","# the result denotes most task is done in aten::pow"]},{"cell_type":"markdown","id":"948644ce","metadata":{"id":"948644ce"},"source":["### torch.profiler"]},{"cell_type":"code","execution_count":9,"id":"0a4adbd6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0a4adbd6","executionInfo":{"status":"ok","timestamp":1749358723511,"user_tz":420,"elapsed":11742,"user":{"displayName":"Lifan Sun (Steven)","userId":"05795298000904000024"}},"outputId":"9d0013ea-6fd7-4fd5-ddfb-f004428e4a3b"},"outputs":[{"output_type":"stream","name":"stdout","text":["-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n","-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","                                          ProfilerStep*         0.00%       0.000us         0.00%       0.000us       0.000us     228.241ms       129.41%     228.241ms     114.121ms             2  \n","                                            aten::copy_         0.01%      96.878us         9.67%     170.636ms      85.318ms     169.957ms        96.37%     169.957ms      84.978ms             2  \n","                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     169.957ms        96.37%     169.957ms      84.978ms             2  \n","                                              aten::pow         0.01%     203.093us         0.02%     294.099us     147.050us       6.408ms         3.63%       6.408ms       3.204ms             2  \n","void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       6.408ms         3.63%       6.408ms       3.204ms             2  \n","                                          ProfilerStep*         2.94%      51.947ms        99.84%        1.762s     881.066ms       0.000us         0.00%     176.365ms      88.183ms             2  \n","                                            aten::randn         0.00%      61.600us        87.20%        1.539s     769.527ms       0.000us         0.00%       0.000us       0.000us             2  \n","                                            aten::empty         0.00%      76.456us         0.00%      76.456us      38.228us       0.000us         0.00%       0.000us       0.000us             2  \n","                                          aten::normal_        87.19%        1.539s        87.19%        1.539s     769.458ms       0.000us         0.00%       0.000us       0.000us             2  \n","                                               aten::to         0.00%      63.787us         9.68%     170.826ms      42.707ms       0.000us         0.00%     169.957ms      42.489ms             4  \n","                                         aten::_to_copy         0.00%      69.329us         9.67%     170.762ms      85.381ms       0.000us         0.00%     169.957ms      84.978ms             2  \n","                                    aten::empty_strided         0.00%      57.144us         0.00%      57.144us      28.572us       0.000us         0.00%       0.000us       0.000us             2  \n","                                        cudaMemcpyAsync         9.66%     170.478ms         9.66%     170.478ms      85.239ms       0.000us         0.00%       0.000us       0.000us             2  \n","                                  cudaStreamSynchronize         0.00%      60.624us         0.00%      60.624us      30.312us       0.000us         0.00%       0.000us       0.000us             2  \n","                                           aten::square         0.00%      13.715us         0.02%     307.814us     153.907us       0.000us         0.00%       6.408ms       3.204ms             2  \n","                                      aten::result_type         0.00%       4.819us         0.00%       4.819us       2.409us       0.000us         0.00%       0.000us       0.000us             2  \n","                                       cudaLaunchKernel         0.00%      84.167us         0.00%      84.167us      42.084us       0.000us         0.00%       0.000us       0.000us             2  \n","                                  cudaDeviceSynchronize         0.16%       2.910ms         0.16%       2.910ms       2.910ms       0.000us         0.00%       0.000us       0.000us             1  \n","-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","Self CPU time total: 1.765s\n","Self CUDA time total: 176.365ms\n","\n"]}],"source":["# ## Default way to use profiler\n","# with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n","#     for _ in range(10):\n","#         a = torch.square(torch.randn(10000, 10000).cuda())\n","\n","# prof.export_chrome_trace(\"trace.json\")\n","\n","## With warmup and skip\n","# https://pytorch.org/docs/stable/profiler.html\n","\n","# Non-default profiler schedule allows user to turn profiler on and off\n","# on different iterations of the training loop;\n","# trace_handler is called every time a new trace becomes available\n","def trace_handler(prof):\n","    print(prof.key_averages().table(\n","        sort_by=\"self_cuda_time_total\", row_limit=-1))\n","    prof.export_chrome_trace(\"/tmp/test_trace_\" + str(prof.step_num) + \".json\")\n","\n","with torch.profiler.profile(\n","    activities=[\n","        torch.profiler.ProfilerActivity.CPU,\n","        torch.profiler.ProfilerActivity.CUDA,\n","    ],\n","\n","    # In this example with wait=1, warmup=1, active=2, repeat=1,\n","    # profiler will skip the first step/iteration,\n","    # start warming up on the second, record\n","    # the third and the forth iterations,\n","    # after which the trace will become available\n","    # and on_trace_ready (when set) is called;\n","    # the cycle repeats starting with the next step\n","\n","    schedule=torch.profiler.schedule(\n","        wait=1,\n","        warmup=1,\n","        active=2,\n","        repeat=1),\n","    on_trace_ready=trace_handler\n","    # on_trace_ready=torch.profiler.tensorboard_trace_handler('./log')\n","    # used when outputting for tensorboard\n","    ) as p:\n","        for iter in range(10):\n","            torch.square(torch.randn(10000, 10000).cuda())\n","            # send a signal to the profiler that the next iteration has started\n","            p.step()"]},{"cell_type":"markdown","id":"ad267657","metadata":{"id":"ad267657"},"source":["### ncu: NVIDIA Nsight Compute\n"]},{"cell_type":"code","execution_count":10,"id":"fe60ff83","metadata":{"id":"fe60ff83","executionInfo":{"status":"ok","timestamp":1749358723512,"user_tz":420,"elapsed":6,"user":{"displayName":"Lifan Sun (Steven)","userId":"05795298000904000024"}}},"outputs":[],"source":["# ncu --set full -o output $(which python) test.py"]},{"cell_type":"markdown","id":"ecc93d61","metadata":{"id":"ecc93d61"},"source":["## Integrating Custom kernel"]},{"cell_type":"markdown","id":"69bfa817","metadata":{"id":"69bfa817"},"source":["### Triton Kernel"]},{"cell_type":"code","source":["import triton\n","import triton.language as tl\n","\n","@triton.jit\n","def square_kernel(\n","    x_ptr,\n","    output_ptr,\n","    n_elements,\n","    BLOCK_SIZE: tl.constexpr,\n","):\n","  pid = tl.program_id(axis=0)\n","  block_start = pid * BLOCK_SIZE\n","  offsets = block_start + tl.arange(0, BLOCK_SIZE)\n","  x = tl.load(x_ptr + offsets, mask=offsets < n_elements, other=0.0)\n","  output = x * x\n","  tl.store(output_ptr + offsets, output, mask=offsets < n_elements)"],"metadata":{"id":"Z1_hVrL7xn9W","executionInfo":{"status":"ok","timestamp":1749358723564,"user_tz":420,"elapsed":55,"user":{"displayName":"Lifan Sun (Steven)","userId":"05795298000904000024"}}},"id":"Z1_hVrL7xn9W","execution_count":11,"outputs":[]},{"cell_type":"code","source":["def square(x):\n","    n_elements = x.numel()\n","    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n","\n","    output = torch.empty_like(x)\n","\n","    square_kernel[grid](\n","        x,\n","        output,\n","        n_elements,\n","        BLOCK_SIZE=1024,\n","    )\n","\n","    return output"],"metadata":{"id":"z1gtCwD-xwbF","executionInfo":{"status":"ok","timestamp":1749358723612,"user_tz":420,"elapsed":44,"user":{"displayName":"Lifan Sun (Steven)","userId":"05795298000904000024"}}},"id":"z1gtCwD-xwbF","execution_count":12,"outputs":[]},{"cell_type":"code","source":["square(t)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QRpVh8W_yEyt","executionInfo":{"status":"ok","timestamp":1749358725123,"user_tz":420,"elapsed":1513,"user":{"displayName":"Lifan Sun (Steven)","userId":"05795298000904000024"}},"outputId":"ceb298e6-7ce2-4bb5-c148-0ad6a6358d96"},"id":"QRpVh8W_yEyt","execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.0990, 1.7078, 0.3829,  ..., 0.7603, 1.8344, 0.6198],\n","        [0.0065, 0.2429, 0.0070,  ..., 2.5966, 2.1683, 0.6403],\n","        [0.8363, 0.5699, 1.4727,  ..., 1.1237, 4.8443, 0.1980],\n","        ...,\n","        [0.7112, 1.3919, 0.0086,  ..., 0.5383, 0.7787, 0.0115],\n","        [0.3561, 0.0340, 0.2762,  ..., 3.7034, 0.0137, 0.2513],\n","        [0.6512, 0.0190, 0.1780,  ..., 0.8531, 0.2841, 1.0220]],\n","       device='cuda:0')"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["### Triton kernel: use torch.compile as starting point"],"metadata":{"id":"BIFTNDhEIHy_"},"id":"BIFTNDhEIHy_"},{"cell_type":"code","source":["!TORCH_LOGS=\"output_code\" python compiled_square.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uWTOfYfAIHKw","executionInfo":{"status":"ok","timestamp":1749362624123,"user_tz":420,"elapsed":17010,"user":{"displayName":"Lifan Sun (Steven)","userId":"05795298000904000024"}},"outputId":"8de21c36-4eb6-40a5-84f0-4961242b6417"},"id":"uWTOfYfAIHKw","execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] Output code: \n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] # AOT ID: ['0_inference']\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] import torch\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] import math\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] import random\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] import os\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] import tempfile\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] from math import inf, nan\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] from torch._inductor.utils import maybe_profile\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] from torch import device, empty_strided\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] from torch._inductor.codegen.multi_kernel import MultiKernelCall\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] import triton\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] import triton.language as tl\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import (\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code]     grid,\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code]     split_scan_grid,\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code]     grid_combo_kernels,\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code]     start_graph,\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code]     end_graph,\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code]     cooperative_reduction_grid,\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] )\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] \n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] aten = torch.ops.aten\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] inductor_ops = torch.ops.inductor\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] _quantized = torch.ops._quantized\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] async_compile = AsyncCompile()\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] \n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] \n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] # kernel path: /tmp/torchinductor_root/ez/cezpfauk4zn3pp2at57nsatygpm75gndnmmmit3losazyie2urdm.py\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] # Topologically Sorted Source Nodes: [square], Original ATen: [aten.pow]\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] # Source node to ATen node mapping:\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] #   square => pow_1\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] # Graph fragment:\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] #   %pow_1 : [num_users=1] = call_function[target=torch.ops.aten.pow.Tensor_Scalar](args = (%arg0_1, 2), kwargs = {})\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] triton_poi_fused_pow_0 = async_compile.triton('triton_poi_fused_pow_0', '''\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] import triton\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] import triton.language as tl\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] from triton.compiler.compiler import AttrsDescriptor\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] \n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] \n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] @triton_heuristics.pointwise(\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code]     size_hints={'x': 134217728}, \n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code]     filename=__file__,\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code]     triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=40, cc=75, major=7, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1024, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor.from_dict({'arg_properties': {'tt.divisibility': (0, 1, 2), 'tt.equal_to': ()}, 'cls': 'AttrsDescriptor'})]},\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code]     inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_pow_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '9182018CCD6A4F758231D68D0B1E1E23CEBB32E5D78CB36B65791C4EB96774A2', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code]     min_elem_per_thread=0\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] )\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] @triton.jit\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] def triton_poi_fused_pow_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code]     xnumel = 100000000\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code]     xmask = xindex < xnumel\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code]     x0 = xindex\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (x0), xmask)\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code]     tmp1 = tmp0 * tmp0\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code]     tl.store(out_ptr0 + (x0), tmp1, xmask)\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] ''', device_str='cuda')\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] \n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] \n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] async_compile.wait(globals())\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] del async_compile\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] \n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] def call(args):\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code]     arg0_1, = args\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code]     args.clear()\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code]     assert_size_stride(arg0_1, (10000, 10000), (10000, 1))\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code]     with torch.cuda._DeviceGuard(0):\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code]         torch.cuda.set_device(0)\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code]         buf0 = empty_strided_cuda((10000, 10000), (10000, 1), torch.float32)\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code]         # Topologically Sorted Source Nodes: [square], Original ATen: [aten.pow]\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code]         stream0 = get_raw_stream(0)\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code]         triton_poi_fused_pow_0.run(arg0_1, buf0, 100000000, grid=grid(100000000), stream=stream0)\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code]         del arg0_1\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code]     return (buf0, )\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] \n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] \n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code]     from torch._inductor.utils import print_performance\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code]     arg0_1 = rand_strided((10000, 10000), (10000, 1), device='cuda:0', dtype=torch.float32)\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code]     fn = lambda: call([arg0_1])\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] \n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] \n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] if __name__ == \"__main__\":\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)\n","V0608 06:03:41.240000 50357 torch/_inductor/graph.py:2045] [0/0] [__output_code] \n","V0608 06:03:41.317000 50357 torch/_inductor/graph.py:2053] [0/0] [__output_code] Output code written to: /tmp/torchinductor_root/tc/ctceo3wijv5g6qb53s22rbl2oaydx5fkjpeiflcsi3ig2xmphsn2.py\n","I0608 06:03:42.174000 50357 torch/_inductor/graph.py:2087] [0/0] [__output_code] Output code written to: /tmp/torchinductor_root/tc/ctceo3wijv5g6qb53s22rbl2oaydx5fkjpeiflcsi3ig2xmphsn2.py\n"]}]},{"cell_type":"markdown","id":"2d67cb09","metadata":{"id":"2d67cb09"},"source":["### Load inline"]},{"cell_type":"code","source":["from torch.utils.cpp_extension import load_inline"],"metadata":{"id":"4enGKHHJyeaw","executionInfo":{"status":"ok","timestamp":1749358725590,"user_tz":420,"elapsed":465,"user":{"displayName":"Lifan Sun (Steven)","userId":"05795298000904000024"}}},"id":"4enGKHHJyeaw","execution_count":14,"outputs":[]},{"cell_type":"code","source":["cpp_source = \"\"\"\n","std::string hello() {\n","    return \"Hello World!\";\n","}\n","\"\"\""],"metadata":{"id":"msZ0_UkRysj5","executionInfo":{"status":"ok","timestamp":1749359385654,"user_tz":420,"elapsed":4,"user":{"displayName":"Lifan Sun (Steven)","userId":"05795298000904000024"}}},"id":"msZ0_UkRysj5","execution_count":20,"outputs":[]},{"cell_type":"code","source":["!ls\n","!mkdir -p tmp"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SwwdjNr54eUe","executionInfo":{"status":"ok","timestamp":1749359386951,"user_tz":420,"elapsed":210,"user":{"displayName":"Lifan Sun (Steven)","userId":"05795298000904000024"}},"outputId":"16bf7ed4-c241-4bbf-de31-57256d213241"},"id":"SwwdjNr54eUe","execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["ncu_prof.log  notes.ipynb  test.py  tmp\n"]}]},{"cell_type":"code","source":["hello_module = load_inline(name='hello_module', cpp_sources=[cpp_source], verbose=True, functions=['hello'], build_directory='./tmp')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_CKDappZ06rl","executionInfo":{"status":"ok","timestamp":1749359407261,"user_tz":420,"elapsed":19176,"user":{"displayName":"Lifan Sun (Steven)","userId":"05795298000904000024"}},"outputId":"b8c7cf0c-10d8-4504-8a21-ca1fd71deb29"},"id":"_CKDappZ06rl","execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["The input conditions for extension module hello_module have changed. Bumping to version 3 and re-building as hello_module_v3...\n","Emitting ninja build file ./tmp/build.ninja...\n","Building extension module hello_module_v3...\n","Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n","Loading extension module hello_module_v3...\n"]}]},{"cell_type":"code","source":["hello_module.hello()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"BvEIKHVu8rQt","executionInfo":{"status":"ok","timestamp":1749359448935,"user_tz":420,"elapsed":10,"user":{"displayName":"Lifan Sun (Steven)","userId":"05795298000904000024"}},"outputId":"e735b4b4-245c-4b70-a85b-dd46dc63b6b0"},"id":"BvEIKHVu8rQt","execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Hello World!'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["## Load inline: continue"],"metadata":{"id":"opBKW6VD-PMh"},"id":"opBKW6VD-PMh"},{"cell_type":"code","source":["cuda_source = \"\"\"\n","__global__ void square_kernel(float *x, float * output, int H, int W) {\n","    int xid = blockIdx.x * blockDim.x + threadIdx.x;\n","    int yid = blockIdx.y * blockDim.y + threadIdx.y;\n","    if (xid < W && yid < H) {\n","        output[yid * W + xid] = x[yid * W + xid] * x[yid * W + xid];\n","    }\n","}\n","\n","torch::Tensor square_matrix(torch::Tensor x) {\n","    const int height = x.size(0);\n","    const int width = x.size(1);\n","    auto result = torch::empty_like(x);\n","    dim3 threads_per_block(16, 16);\n","    dim3 number_of_blocks((width + threads_per_block.x - 1) / threads_per_block.x,\n","                          (height + threads_per_block.y - 1) / threads_per_block.y);\n","    square_kernel<<<number_of_blocks, threads_per_block>>>(x.data_ptr<float>(), result.data_ptr<float>(), height, width);\n","    return result;\n","}\n","\"\"\"\n","\n","cpp_source = \"\"\"\n","torch::Tensor square_matrix(torch::Tensor);\n","\"\"\""],"metadata":{"id":"RVGClvXv-Srz","executionInfo":{"status":"ok","timestamp":1749362330461,"user_tz":420,"elapsed":10,"user":{"displayName":"Lifan Sun (Steven)","userId":"05795298000904000024"}}},"id":"RVGClvXv-Srz","execution_count":28,"outputs":[]},{"cell_type":"code","source":["!mkdir -p tmp_cuda"],"metadata":{"id":"dGkHPAZwHFbK","executionInfo":{"status":"ok","timestamp":1749362174621,"user_tz":420,"elapsed":103,"user":{"displayName":"Lifan Sun (Steven)","userId":"05795298000904000024"}}},"id":"dGkHPAZwHFbK","execution_count":26,"outputs":[]},{"cell_type":"code","source":["my_module = load_inline(name='my_module', cpp_sources=[cpp_source], cuda_sources=[cuda_source], verbose=True, functions=['square_matrix'], build_directory='./tmp_cuda')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zpF-oVYoG1S_","executionInfo":{"status":"ok","timestamp":1749362377212,"user_tz":420,"elapsed":44107,"user":{"displayName":"Lifan Sun (Steven)","userId":"05795298000904000024"}},"outputId":"6d42b58e-964c-4568-c0a7-082e28e6b79f"},"id":"zpF-oVYoG1S_","execution_count":29,"outputs":[{"output_type":"stream","name":"stderr","text":["The input conditions for extension module my_module have changed. Bumping to version 1 and re-building as my_module_v1...\n","Detected CUDA files, patching ldflags\n","Emitting ninja build file ./tmp_cuda/build.ninja...\n","/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n","If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n","  warnings.warn(\n","Building extension module my_module_v1...\n","Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n","Loading extension module my_module_v1...\n"]}]},{"cell_type":"code","source":["my_module.square_matrix(t)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BGHVgJBNHDam","executionInfo":{"status":"ok","timestamp":1749362393825,"user_tz":420,"elapsed":10,"user":{"displayName":"Lifan Sun (Steven)","userId":"05795298000904000024"}},"outputId":"41019f37-3fb0-4b4d-dc02-5eeae84cac8b"},"id":"BGHVgJBNHDam","execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.0990, 1.7078, 0.3829,  ..., 0.7603, 1.8344, 0.6198],\n","        [0.0065, 0.2429, 0.0070,  ..., 2.5966, 2.1683, 0.6403],\n","        [0.8363, 0.5699, 1.4727,  ..., 1.1237, 4.8443, 0.1980],\n","        ...,\n","        [0.7112, 1.3919, 0.0086,  ..., 0.5383, 0.7787, 0.0115],\n","        [0.3561, 0.0340, 0.2762,  ..., 3.7034, 0.0137, 0.2513],\n","        [0.6512, 0.0190, 0.1780,  ..., 0.8531, 0.2841, 1.0220]],\n","       device='cuda:0')"]},"metadata":{},"execution_count":30}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}